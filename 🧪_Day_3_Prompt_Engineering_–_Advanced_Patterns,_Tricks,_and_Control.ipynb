{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHiHNOfUCcT5",
        "outputId": "8dc46546-46cc-4935-e238-ead4a6b83ae7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Few Shot**"
      ],
      "metadata": {
        "id": "ZSxDdDAEDVJZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdLjnZbKBjxv",
        "outputId": "c79e80a3-43b8-4a09-8b00-3872ce4aa15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is AI?\n",
            "A: AI stands for Artificial Intelligence. It is a field of computer science that focuses on making machines intelligent.\n",
            "\n",
            "Q: What is ML?\n",
            "A:\n",
            " (a) A term used in the 1950s and 1960's to describe machine learning techniques which was developed at IBM Research Laboratories by Dr Walter Baudelaire, who invented computers with data structures similar or better than those found today such as words like \"speech\" or \".edu\". He later claimed he had written it off because humans were so quick-witted about how they could make them think logically correct when done right; hence his name 'ML' The technology underpinning this work has been called Machine Learning since its inception after an episode described below - but why did people call some applications artificially intelligence using these tricks artificial names before human beings do too much typing away long lines from their keyboards without prompting you if your idea sounds interesting?! And what exactly does NLP mean anyway... let me explain again : there are different ways through word processors can be understood within speech language processing programs including automated voice recognition systems based upon neural network theory [see article] So simply put, all kinds of programming languages should have semantic semantics matching one another. As I said previously i am not talking about any particular specific design process here although many examples exist already -- especially something we know works well enough under various conditions e.... If anyone really wanted to understand more then just see my blog post  Here\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "def prompt_test(prompt):\n",
        "  output = generator(\n",
        "      prompt,\n",
        "      max_length=50,\n",
        "      temperature=0.8,\n",
        "      top_k=80,\n",
        "      top_p=0.92,\n",
        "      num_return_sequences=1,\n",
        "      repetition_penalty=1.3,\n",
        "      pad_token_id=50256\n",
        "  )\n",
        "  return output[0][\"generated_text\"]\n",
        "\n",
        "# Few Shot\n",
        "prompt = \"\"\"\n",
        "Q: What is AI?\n",
        "A: AI stands for Artificial Intelligence. It is a field of computer science that focuses on making machines intelligent.\n",
        "\n",
        "Q: What is ML?\n",
        "A:\n",
        "\"\"\"\n",
        "print(prompt_test(prompt))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"You are a computer science professor. Explain the concept of neural networks in simple words:\"\n",
        "print(prompt_test(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x6jfFKeCUkW",
        "outputId": "5bf52ce2-d97e-461a-c934-c455cfc0fe81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a computer science professor. Explain the concept of neural networks in simple words: It's not just one thing, it is all about everything and you can't do that if your brain doesn\n",
            "This article originally appeared on Psychology Today.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Solve this problem step-by-step: If Yasir has 3 AI models and he fine-tunes each one for 2 hours, how many hours in total?\"\n",
        "print(prompt_test(prompt))"
      ],
      "metadata": {
        "id": "1hUcrT6nFS9t",
        "outputId": "686d1d88-ce8d-40fe-ac3c-1dba67866416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solve this problem step-by-step: If Yasir has 3 AI models and he fine-tunes each one for 2 hours, how many hours in total?\n",
            "The question is simple. We need to find a way to count the number of time intervals between changes that have been performed using our standard method (e.-g., every 10+ years). A computer program can do it quickly but requires several minutes or so on average per update/update cycle as well; however most likely you would not want an algorithm which computes more than 8 updates over such cycles within 1 minute by simply doing math calculations instead! Let's start with measuring what interval we are going after here again because all other systems use different methods also!! And let us add something about performance since there isn't much reason to trust your system when trying these two things at once without having any prior experience working through them together just yet! So if they get out into traffic right away then maybe even faster speed up their data transfer rate due either direct comparison against previous times versus higher frequency comparisons from new points somewhere else…and thus speeding down work flow rates substantially :D ;-) I mean now some people don�d like learning numbers - especially bad ones – while others appreciate its efficiency...but doesnt see why getting results \"just\" 15 seconds above nonnegative errors may be worth saving $1000 USD later?! For example consider applying 100% accuracy scaling around 50K moves\n"
          ]
        }
      ]
    }
  ]
}